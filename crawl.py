import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
import datetime, time
from time import localtime, strftime
from datetime import timedelta
from tqdm import trange
import time

import re

def page_scrapping(url_pre, page_no):
    page_no_str = str(page_no)
    url = url_pre + page_no_str
    response = requests.get(url)
    html = BeautifulSoup(response.text, 'html.parser')
    content_link = []
    for i in html.select('td.tl > div > a'):
        content_link.append(i['href'])
    df = pd.read_html(url)[0]
    df = df[df['ì—…ë¬´'].notnull()]
    
    return df

def basic_amount(xpath):
    try: 
        basic_amount_button = driver.find_element(By.XPATH, xpath)
        basic_amount_button.click()

        # ìƒˆ ì°½/íŒì—…ìœ¼ë¡œ ì „í™˜ (ìƒˆ ì°½/íŒì—…ì´ ì—´ë¦¬ëŠ” ê²½ìš°)
        driver.switch_to.window(driver.window_handles[-1])

        # ìƒˆ í˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•´ ëŒ€ê¸°
        time.sleep(0.1)  # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¶©ë¶„í•œ ì‹œê°„ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.

        # í˜„ì¬ í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        html_source = driver.page_source

        # í˜ì´ì§€ì˜ ëª¨ë“  í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ì¶”ì¶œ
        tables = pd.read_html(html_source)


        pre_price_basic = tables[0].iloc[2,3]       # ì˜ˆë¹„ê°€ê²© ê¸°ì´ˆê¸ˆì•¡
        pre_price_basic_cost = tables[0].iloc[3,3]  # ì˜ˆë¹„ê°€ê²© ê¸°ì´ˆê¸ˆì•¡ ì¤‘ ìˆœê³µì‚¬ì›ê°€
        a_value = tables[0].iloc[5,3]  # A ê¸ˆì•¡

        # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìˆ«ìë§Œ ì¶”ì¶œ
        # \d+ ëŠ” í•˜ë‚˜ ì´ìƒì˜ ìˆ«ìë¥¼ ì˜ë¯¸í•˜ë©°, ì‰¼í‘œ(,)ì™€ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
        pre_price_basic_temp = re.sub(r'[^\d]', '', pre_price_basic.split('(')[0])
        pre_price_basic_cost_temp = re.sub(r'[^\d]', '', pre_price_basic_cost.split('ì›')[0])
        a_value_temp = re.sub(r'[^\d]', '', a_value.split('ì›')[0])

        # ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜

        pre_price_basic_int = int(pre_price_basic_temp)
        pre_price_basic_cost_int = int(pre_price_basic_cost_temp)
        a_value_temp_int = int(a_value_temp)

        # ì„¸ê°œì˜ ê°’ì„ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€

        return {
            'ì˜ˆë¹„ê°€ê²© ê¸°ì´ˆê¸ˆì•¡': pre_price_basic_int,
            'ì˜ˆë¹„ê°€ê²© ê¸°ì´ˆê¸ˆì•¡ ì¤‘ ìˆœê³µì‚¬ì›ê°€': pre_price_basic_cost_int,
            'A ê¸ˆì•¡': a_value_temp_int
            }
    
    except NoSuchElementException:
        return None

def open_bid(xpath):
    try: 
        ### 1) ê°œì°°ì™„ë£Œ > ì…ì°° ì—…ì²´ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ í›„ 1ìˆœìœ„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        open_bid_button = driver.find_element(By.XPATH, xpath)
        open_bid_button.click()

        # ìƒˆ ì°½/íŒì—…ìœ¼ë¡œ ì „í™˜ (ìƒˆ ì°½/íŒì—…ì´ ì—´ë¦¬ëŠ” ê²½ìš°)
        driver.switch_to.window(driver.window_handles[-1])

        # ìƒˆ í˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•´ ëŒ€ê¸°
        time.sleep(0.1)  # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¶©ë¶„í•œ ì‹œê°„ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.

        # í˜„ì¬ í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        html_source_complete = driver.page_source

        # í˜ì´ì§€ì˜ ëª¨ë“  í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ì¶”ì¶œ
        tables_complete = pd.read_html(html_source_complete)

        biz_number = None
        company_name = None
        representative_name = None
        bid_amount = None
        bid_rate = None
        bid_difference = None
            
        # ì¼ë°˜ì ì¸ ê²½ìš°, í…Œì´ë¸”ì´ 2ê°œ
        if len(tables_complete) == 2 and len(tables_complete[1].columns) > 4:
            biz_number = tables_complete[1].iloc[0, 1]
            company_name = tables_complete[1].iloc[0, 2]
            representative_name = tables_complete[1].iloc[0, 3]
            bid_amount = tables_complete[1].iloc[0, 4]
            bid_rate = tables_complete[1].iloc[0, 5]
            bid_difference = tables_complete[1].iloc[0, 6]
        
        # ì—…ì²´ê°€ í¬ê¸°í–ˆì„ ê²½ìš°, í…Œì´ë¸”ì´ í•˜ë‚˜ê°€ ëŠ˜ì–´ë‚¨
        elif len(tables_complete) == 3 and len(tables_complete[2].columns) > 4:
            biz_number = tables_complete[2].iloc[0, 1]
            company_name = tables_complete[2].iloc[0, 2]
            representative_name = tables_complete[2].iloc[0, 3]
            bid_amount = tables_complete[2].iloc[0, 4]
            bid_rate = tables_complete[2].iloc[0, 5]
            bid_difference = tables_complete[2].iloc[0, 6]
            
        else:
            biz_number = None
            company_name = None
            representative_name = None
            bid_amount = None
            bid_rate = None
            bid_difference = None
        
        
        pre_button = driver.find_element(By.XPATH, '/html/body/div[2]/div[2]/form[1]/div[2]/table/tbody/tr[5]/td[2]/div/a/span')
                                                    
        pre_button.click()

        # ìƒˆ ì°½/íŒì—…ìœ¼ë¡œ ì „í™˜ (ìƒˆ ì°½/íŒì—…ì´ ì—´ë¦¬ëŠ” ê²½ìš°)
        driver.switch_to.window(driver.window_handles[-1])

        # ìƒˆ í˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•´ ëŒ€ê¸°
        time.sleep(0.1)  # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¶©ë¶„í•œ ì‹œê°„ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.

        # í˜„ì¬ í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        html_source = driver.page_source

        # í˜ì´ì§€ì˜ ëª¨ë“  í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ì¶”ì¶œ
        tables = pd.read_html(html_source)
        
        estimated_price = None
        estimated_price_cost = None
        
        if len(tables) > 2 and len(tables[2]) > 0 and len(tables[2].columns) > 1:
            estimated_price = tables[2].iloc[0,1] # ì˜ˆì •ê°€ê²©
        if len(tables) > 3 and len(tables[3]) > 0 and len(tables[3].columns) > 1:
            estimated_price_cost = tables[3].iloc[0,1] # ì˜ˆì •ê°€ê²© ì¤‘ ìˆœê³µì‚¬ì›ê°€
        
#         estimated_price = tables[2].iloc[0,1] # ì˜ˆì •ê°€ê²©
#         estimated_price_cost = tables[3].iloc[0,1] # ì˜ˆì •ê°€ê²© ì¤‘ ìˆœê³µì‚¬ì›ê°€
            
        return {
            'ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸': biz_number,
            'ì—…ì²´ëª…': company_name,
            'ëŒ€í‘œìëª…': representative_name,
            'ì…ì°°ê¸ˆì•¡(ì›)': bid_amount,
            'íˆ¬ì°°ë¥ (%)': bid_rate,
            '(ì…ì°°ê°€ê²©-A)(ì˜ˆì •ê°€ê²©-A) *100': bid_difference,
            'ì˜ˆì •ê°€ê²©': estimated_price,
            'ì˜ˆì •ê°€ê²© ì¤‘ ìˆœê³µì‚¬ì›ê°€': estimated_price_cost # ì˜ˆì •ê°€ê²© ì¤‘ ìˆœê³µì‚¬ì›ê°€
            }
    
    except NoSuchElementException:
        return None

def run_scraper(input_date, input_days):

    low_limit = 500000000
    high_limit = 15000000000

    # ì…ë ¥ëœ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
    d = datetime.datetime.strptime(input_date, '%Y%m%d').date()

    # ì…ë ¥ëœ ì¼ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ timedelta ê°ì²´ ìƒì„±
    td = timedelta(days=input_days)

    # ê³„ì‚°ëœ ê¸°ê°„ìœ¼ë¡œ fromBidDtì™€ toBidDt ì„¤ì •
    fromBidDt = "20" + (d - td).strftime("%y/%m/%d")
    toBidDt = "20" + d.strftime("%y/%m/%d")

    print("ì¡°íšŒ ì‹œì‘ ë‚ ì§œ:", fromBidDt)
    print("ì¡°íšŒ ì¢…ë£Œ ë‚ ì§œ:", toBidDt)

    # URL ì„¤ì •
    url_one = 'https://www.g2b.go.kr:8101/ep/tbid/tbidList.do?area=11&areaNm=&bidNm=&bidSearchType=1&budgetCompare=&detailPrdnm=&detailPrdnmNo=&fromBidDt='
    url_two = '&fromOpenBidDt=&industry=&industryCd=0002&instNm=&instSearchRangeType=&intbidYn=&orgArea=&procmntReqNo=&radOrgan=1&recordCountPerPage=100&refNo=&regYn=Y&searchDtType=1&searchType=1&strArea=&taskClCds=&toBidDt='
    url_three = '&toOpenBidDt=&useTotalCount=Y&currentPageNo='

    url_pre = url_one + fromBidDt + '&upBudget=' + str(low_limit) +'&downBudget=' + str(high_limit) + url_two + toBidDt + url_three

    print(url_pre)
    # ì…ì°°ê³µê³  ì¡°íšŒì‹œ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
    response = requests.get(url_pre + str(1)) 
    html = BeautifulSoup(response.text, 'html.parser')

    try : 
        last_page_no = html.select('#pagination > a.next_end')[0]['href'].split('=')[-1]

    except :
        last_page_no = 1 # ë˜ëŠ” ì ì ˆí•œ ê¸°ë³¸ê°’ ì„¤ì •
        
    print(last_page_no) # ì…ì°°ê³µê³  ì¡°íšŒì‹œ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸

    # Selenium ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=chrome_options)

    basic_data = pd.DataFrame({col: [] for col in page_scrapping(url_pre, 1).columns}) # # ì»¬ëŸ¼ëª…ì€ ì‚¬ìš©í•˜ë˜ ê°’ì€ ë¹„ì›Œë‘” ìƒˆ DataFrame ìƒì„±
    additional_data = [] # ê¸°ì´ˆê¸ˆì•¡ì¡°íšŒ ê´€ë ¨
    completed_data = [] # ê°œì°°ì™„ë£Œ ê´€ë ¨
    expected_data = [] # ê°œì°°ì™„ë£Œ > ì˜ˆì •ê°€ê²© ê´€ë ¨

    for i in trange(1, (int(last_page_no)+1)):
        
        # ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
        df = page_scrapping(url_pre, i)
        time.sleep(0.1) # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¶©ë¶„í•œ ì‹œê°„ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        # Extracting the dates into separate columns
        df[['ê³µê³ ì¼ì', 'ì…ì°°ì¼ì']] = df['ì…ë ¥ì¼ì‹œ (ì…ì°°ë§ˆê°ì¼ì‹œ)'].str.extract(r'(\d{4}/\d{2}/\d{2}) \d{2}:\d{2} \((\d{4}/\d{2}/\d{2}) \d{2}:\d{2}\)')
        
        basic_data = pd.concat([basic_data, df], ignore_index = True)

        
        
    for url in basic_data['ë‚´ìš©ë§í¬']:

        # ê¸°ì´ˆê¸ˆì•¡ì¡°íšŒ ë²„íŠ¼ í´ë¦­
        driver.get(url)
        result_basic = basic_amount('/html/body/div[2]/div[2]/div[16]/table/tbody/tr/td[4]/a/span')

        if result_basic is None:
            result_basic = basic_amount('/html/body/div[2]/div[2]/div[17]/table/tbody/tr/td[4]/a/span')
        if result_basic is None:
            result_basic = basic_amount('/html/body/div[2]/div[2]/div[18]/table/tbody/tr/td[4]/a/span')
            
        if result_basic is None:
            result_basic = {
                'ì˜ˆë¹„ê°€ê²© ê¸°ì´ˆê¸ˆì•¡': None,
                'ì˜ˆë¹„ê°€ê²© ê¸°ì´ˆê¸ˆì•¡ ì¤‘ ìˆœê³µì‚¬ì›ê°€': None,
                'A ê¸ˆì•¡': None
            }
        additional_data.append(result_basic)

        # ê°œì°°ì™„ë£Œ ë°ì´í„° 

    for url in basic_data['ë‚´ìš©ë§í¬']:
        driver.get(url)
        result = open_bid('/html/body/div[2]/div[2]/div[24]/table/tbody/tr/td[5]/a')
        if result is None:
            result = open_bid('/html/body/div[2]/div[2]/div[25]/table/tbody/tr/td[5]/a/span')
        if result is None:
            result = open_bid('/html/body/div[2]/div[2]/div[26]/table/tbody/tr/td[5]/a/span')

        if result is None:
            result = open_bid('//button[text()="ê°œì°°ì™„ë£Œ"]')
        if result is None:
            result = {
                'ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸': None,
                'ì—…ì²´ëª…': None,
                'ëŒ€í‘œìëª…': None,
                'ì…ì°°ê¸ˆì•¡(ì›)': None,
                'íˆ¬ì°°ë¥ (%)': None,
                '(ì…ì°°ê°€ê²©-A)(ì˜ˆì •ê°€ê²©-A) *100': None,
                'ì˜ˆì •ê°€ê²©': None,
                'ì˜ˆì •ê°€ê²© ì¤‘ ìˆœê³µì‚¬ì›ê°€': None
            }
        completed_data.append(result)

    driver.quit()

    # ì„¸ê°œ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    additional_rows_df = pd.DataFrame(additional_data)
    #expected_data_df = pd.DataFrame(expected_data)
    completed_data_df = pd.DataFrame(completed_data)

    # ë„¤ê°œ DataFrameì˜ ì¸ë±ìŠ¤ë¥¼ ë¦¬ì…‹
    basic_data_reset = basic_data.reset_index(drop=True)
    #expected_data_df = expected_data_df.reset_index(drop=True)
    additional_rows_df_reset = additional_rows_df.reset_index(drop=True)
    completed_data_df_reset = completed_data_df.reset_index(drop=True)

    # ì¸ë±ìŠ¤ë¥¼ ë¦¬ì…‹í•œ DataFrameì„ ë³‘í•©
    combined_df = pd.concat([basic_data_reset, additional_rows_df_reset, completed_data_df_reset ], axis=1) # expected_data_df,
    return combined_df

# Download link for the data
st.success('ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!')
st.download_button(label='ğŸ“¥ ë‹¤ìš´ë¡œë“œ: Excel íŒŒì¼', 
                    data=towrite, 
                    file_name='result.xlsx', 
                    mime='application/vnd.ms-excel')


st.title("ê³µê³ ì¼ì ê¸°ì¤€ ê³µê³  ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ")

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
input_date = st.text_input("ì¡°íšŒ ì¢…ë£Œì¼ìë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 20231212): ")
input_days = st.number_input("ì¡°íšŒ ê¸°ê°„(ì¼ìˆ˜)ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 3): ", min_value=1, value=3)

if st.button('ì¡°íšŒí•˜ê¸°'):
    result_df = run_scraper(input_date, input_days)
    st.dataframe(result_df)
    st.download_button(label='ğŸ“¥ ë‹¤ìš´ë¡œë“œ: Excel íŒŒì¼', data=result_df.to_excel().encode('utf-8'), file_name='data.xlsx', mime='application/vnd.ms-excel')

    